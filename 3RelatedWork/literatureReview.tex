\chapter{Related Works}
This chapter provides a comprehensive review of the existing body of literature that informs and supports the research presented in this thesis. It delves into the domains of large language models (LLMs), enhancing retrieval pipelines and evaluating retrieval systems. Furthermore, it examines the concept of query variation, shedding light on benchmark datasets, their impact on system performance, and the various methods employed for generating query variations. This chapter aims to present a detailed overview of the current state of knowledge in the field and identify the specific gaps and research areas this study seeks to address.

Query variations, as explored in this chapter, can originate from multiple sources, including the ambiguity in user search terms, diverse user intentions, and variations in the search context. This chapter meticulously evaluates the existing literature on query variation generators and their integration within LLMs, aiming to uncover the current state of research in this critical area and to pinpoint any notable gaps or opportunities for future investigation.

\section{Information Retrieval}
Recent studies in Information Retrieval (IR) have been driven by a growing interest in enhancing the performance and efficiency of LLMs in Natural Language Processing (NLP) tasks, particularly text generation and understanding. These studies leverage pre-trained models like BERT \cite{bert} and GPT \cite{gpt}, which have consistently achieved state-of-the-art results across a range of NLP tasks. Pre-trained models are a foundation for various NLP applications, facilitating their adaptability and effectiveness.

One noteworthy trend in this domain is exploring transfer learning techniques, such as fine-tuning \cite{bert} and domain adaptation \cite{wu}. Fine-tuning allows practitioners to customise pre-trained LLMs for specific tasks or domains, while domain adaptation helps models perform optimally in specialised contexts. These techniques are critical contributors to the versatility of LLMs in addressing real-world challenges.

Looking ahead, the research landscape is poised to delve into more complex and diverse NLP tasks, extending beyond traditional text generation and classification. Researchers aim to develop innovative methods to enhance the efficiency and scalability of LLMs further, making them applicable to an even more comprehensive array of domains and applications \cite{bailey, zuccon, benham}.

\subsection{Ranking Models}
In recent literature, a multitude of models has risen to prominence, each wielding unique roles and making substantial contributions to the domains of information retrieval and natural language processing. Within the scope of this study, the selected models serve specific purposes. In an investigation conducted by Askari et al., the injection of BM25 (Best Matching) scores into the input of cross-encoder re-rankers, mirroring the original query structure, notably bolstered their overall efficacy \cite{bminitial}. Relevance Model 3 (RM3) stands out as a primary tool for query expansion, effectively augmenting the initial query with supplementary terms to elevate retrieval performance and enhance the quality of search results \cite{rm3}.

Kernelised Neural Ranking Models (KNRM) and Convolutional Kernelised Neural Ranking Models (CKNRM) have become essential for semantic matching and ranking, leveraging neural networks to encapsulate the semantic relationships between queries and documents, ultimately yielding more precise rankings \cite{knrm, cknrm}. In context-aware passage retrieval, EPIC (Efficient Passage Indexing via Contextualisation) emerges as a critical asset. Its ability to seamlessly integrate contextual information enables the retrieval of passages most relevant within a given context  \cite{epic, wray}. 

Furthermore, BERT (Bidirectional Encoder Representations from Transformers) and T5 (Text-to-Text Transfer Transformer) have risen to prominence as pre-trained transformer-based models applicable to an array of NLP tasks. These models serve as pivotal instruments for investigation by establishing a robust foundation for transfer learning. Researchers can fine-tune them to address specific tasks, consistently achieving state-of-the-art performance across a spectrum of natural language understanding and generation tasks. The advent of BERT and T5 has significantly revolutionised the field of NLP, bringing about notable advancements in text-based information retrieval systems \cite{bert, t5}.

\subsection{Improvement of Retrieval Pipelines}
The recent literature surrounding retrieval pipelines has been marked by a concerted effort to elevate the effectiveness and efficiency of handling large-scale datasets and accommodating diverse query types. Researchers have embraced deep learning techniques, such as deep reinforcement learning and neural networks, to optimise the search process \cite{penha2022, bailey, gao}. These techniques allow for the development of more sophisticated retrieval systems that can adapt to different contexts and user needs.

Another significant area of investigation pertains to query expansion and query prediction techniques, which are crucial in improving the accuracy and relevance of search results \cite{zendel, lu, moffat}. By enhancing query understanding and prediction, retrieval systems become more adept at delivering meaningful and targeted search results to users.

The development of novel evaluation metrics, including diversity measures, represents a significant stride forward in assessing the quality of search results. These metrics go beyond traditional accuracy-based measures and focus on determining the relevance, novelty, and richness of retrieved information. This holistic approach to evaluation, as seen in Sokolova et al.'s work \cite{sokolova}, is essential for ensuring that retrieval pipelines truly enhance user satisfaction and the overall search experience.

While numerous studies are dedicated to improving retrieval pipelines, there is a concurrent focus on identifying and addressing vulnerabilities and weaknesses in these systems. Gao et al.'s work \cite{gao}, which introduces a framework designed to generate misleading queries, highlights the importance of robustness and security in retrieval systems. Identifying and mitigating potential weaknesses is essential for developing reliable and trustworthy retrieval pipelines.

\subsection{Evaluation}
Recent literature on evaluating LLMs emphasises the need for more diverse and robust evaluation methods to gauge their performance across various NLP tasks accurately. Traditional metrics like accuracy, which assess a model's ability to classify text correctly, have been scrutinised for their applicability in real-world scenarios \cite{sokolova, ribeiro}. Gardner et al. proposed the creation of contrast sets to evaluate NLP models, showing that model effectiveness on these sets can be substantially lower than on the original test sets \cite{gardner}.

Moffat et al.'s introduction of the C/W/L framework \cite{moffat} has presented a novel approach to evaluation that takes into consideration user behaviour and engagement. This framework factors the likelihood of a user continuing to the following document while exploring search results, providing a more nuanced and user-centric evaluation of LLMs' performance. This approach aims to capture the "expected goal of search," ultimately leading to more accurate predictions of user intent and needs.

Ribeiro et al.'s proposed evaluation model \cite{ribeiro} offers an innovative perspective by addressing the limitations of conventional evaluation measures and methods. Their experiments have uncovered previously unnoticed shortcomings in NLP models that had undergone extensive testing. This highlights the importance of continually evolving evaluation methodologies to ensure that LLMs are rigorously assessed and refined for real-world applications.

\section{Query Variations}
\subsection{Benchmark Datasets}
The significance of query variations in benchmark datasets has been increasingly emphasised in recent studies, including those by Penha et al. \cite{penha2022}, Zendel et al. \cite{zendel}, and Lu et al. \cite{lu}. The inclusion of diverse query variations is critical for several reasons. Firstly, it enhances the realism of the datasets, making them more reflective of the types of queries that users employ in real-world scenarios. Additionally, incorporating query variations ensures that benchmark datasets cover a wide spectrum of queries and information needs, contributing to a more comprehensive evaluation of the effectiveness of IR systems in handling diverse user queries \cite{penha2022, zendel, characterbert}. To cater to these requirements, some datasets have been specifically designed, such as UQV100 \cite{uqv}, TREC \cite{trec}, and the more recent addition, DL-TYPO \cite{characterbert}.

Of particular note is the study by Zhuang et al., which focuses on the challenges faced by current dense retrievers in handling atypical or unusual queries. This underscores the practical relevance of benchmark datasets containing query variations and the need to address such challenges.

Furthermore, Penha et al.'s research \cite{penha2022}, as highlighted in their investigation into the robustness of retrieval pipelines with datasets featuring query variations, reveals that using these benchmark datasets can stimulate improvements in IR systems by pinpointing areas of weakness. These studies have collectively initiated the advancement of this field, and the experiments in this study aim to build upon their research. In conclusion, including query variations in benchmark datasets is paramount for evaluating and enhancing the performance of IR systems in effectively managing variations in user queries and information needs.

\subsection{Effect on Performance}
The research on query variation is indispensable because it is firmly established that such variations have a detrimental impact on the efficacy of LLMs in IR—numerous studies, including those by Chen et al. \cite{chen}, Zhuang et al. \cite{characterbert}, and Ribeiro et al. \cite{ribeiro}, have consistently shown that text variations lead to underperforming IR systems.

A noteworthy contribution comes from Penha et al.'s study \cite{penha2022}, which modelled seven approaches to ranking using two datasets. Their findings demonstrate that when faced with four different query variations, the effectiveness of IR systems dropped by an average of 20\%. This underscores the pressing need for further research and developing strategies to facilitate the existing models to handle query variations better.

Additionally, Lu et al.'s research \cite{lu} on relevance modelling with multiple queries provides valuable insights into how the utilisation of query variations can significantly outperform the use of a single query. Their experiments involved fusion at the term, query, and document levels, introducing a novel concept in the field that shows promising results. By incorporating query variations, LLMs can capture the nuances and complexities of user queries and information needs more effectively, resulting in more accurate and relevant search results, as supported by several recent studies \cite{penha2022, zendel, lu, characterbert}. Moreover, Zuccon et al.'s study \cite{zuccon} is notable for examining the role of query variations in comparing system effectiveness and proposing a framework that explicitly incorporates query variations. Unlike similar studies, their analysis takes into account not only the mean efficacy of the system but also its variance across different query variations and topics, revealing a significant impact of query variations on comparing other systems.

\subsection{Methods of Generation}
Recent research has explored various methods for generating queries that align more closely with user intent and effectively manage variations. These methods include data augmentation, fusion, query reformulation, and adversarial training, as investigated by Lu et al. \cite{lu}, Bailey et al. \cite{bailey}, and Benham et al. \cite{benham}. Bailey et al. emphasise the critical role of query formulation in studying query variability effectively within LLM evaluation. Benham et al. delves into constructing query variations using fusion and weighted random sampling processes, achieving retrieval effectiveness that competes with state-of-the-art approaches. Chakraborty et al. explored the generation of query variations, either through leveraging an external knowledge base with a prior term distribution or by iteratively constructing a relevance model \cite{chakraborty}.

In summary, the recent literature on query variation in IR highlights the critical role of benchmark datasets, the potential impact of query variations on system performance, the promising approach of query generation techniques, and the need for more appropriate evaluation metrics. Accounting for query variations is essential for developing effective IR systems that can accurately understand and respond to the diverse information needs of users.

\section{Relevant Gaps Identified}
\subsubsection{Benchmark Datasets}
While existing literature recognises the importance of query variations in benchmark datasets \cite{penha2022, zendel, lu}, there is limited focus on the DL-TYPO dataset. This dataset is unique in its specific focus on queries containing typos, which is a common occurrence in real-world search queries.
Real-world typos in queries are practical and commonly encountered. The DL-TYPO dataset provides an opportunity to investigate how retrieval systems handle this particular type of query variation \cite{characterbert}. The gap lies in understanding how typos impact retrieval performance and whether methods that work for other query variations are equally effective for typos.
Addressing this gap is essential because typos in user queries pose a distinct challenge. Exploring how retrieval systems perform when faced with typos can provide valuable insights for improving system robustness.

\subsubsection{Effect on Performance}
While the research has established that query variations, in general, negatively affect the effectiveness of LLMs in IR \cite{penha2022, zuccon, ribeiro}, it is crucial to investigate whether LLMs exhibit different performance characteristics when dealing with queries containing typos instead of other types of query variations.
Typos are a common and practical source of query variations, and understanding how these variations impact LLMs' performance is vital. LLMs might respond differently to typos compared to other variations, and these differences need exploration.
Bridging this gap is important because it can provide insights into whether retrieval models need specialised handling when dealing with typos. It may lead to the development of targeted methods for addressing query typos.

\subsubsection{Methods of Generation}
Recent studies discuss various methods for generating query variations \cite{lu, bailey, benham}; however, there is limited coverage of methods explicitly tailored to handle query variations related to typos in user queries.
Generating query variations related to typos requires specific techniques, as typos can have distinct patterns and characteristics. Existing methods may not be optimised for this purpose, and thus, understanding and developing effective methods for typos is a research area with a gap.
Addressing this gap is valuable because it can lead to developing specialised methods for generating typos, which is critical for generating more realistic query variations in benchmarks like DL-TYPO.

In summary, expanding the study to the DL-TYPO dataset can provide an opportunity to address these gaps and contribute to a comprehensive understanding of how real-world typos in queries impact retrieval systems and the development of effective methods for handling typos in query variations.