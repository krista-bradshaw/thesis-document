\section{Query Variations}
\subsection{Benchmark Datasets}
The significance of query variations in benchmark datasets has been increasingly emphasised in recent studies, including those by Penha et al. \cite{penha2022}, Zendel et al. \cite{zendel}, and Lu et al. \cite{lu}. The inclusion of diverse query variations is critical for several reasons. Firstly, it enhances the realism and representativeness of the datasets, making them more reflective of the types of queries that users employ in real-world scenarios. Additionally, incorporating query variations ensures that benchmark datasets cover a wide spectrum of queries and information needs, contributing to a more comprehensive evaluation of the effectiveness of Information Retrieval (IR) systems in handling diverse user queries \cite{penha2022, characterbert, zendel}. To cater to these requirements, some datasets have been specifically designed, such as UQV100 \cite{uqv}, TREC \cite{trec}, and the more recent addition, DL-TYPO \cite{characterbert}.

Of particular note is the study by Zhuang et al., which focuses on the challenges faced by current dense retrievers in handling atypical or unusual queries. This underscores the practical relevance of benchmark datasets containing query variations and the need to address such challenges.

Furthermore, Penha et al.'s research, as highlighted in their investigation into the robustness of retrieval pipelines with datasets featuring query variations, reveals that using these benchmark datasets can stimulate improvements in IR systems by pinpointing areas of weakness. These studies have collectively initiated the advancement of this field, and the experiments in this study aim to build upon their research. In conclusion, including query variations in benchmark datasets is paramount for evaluating and enhancing the performance of IR systems in effectively managing variations in user queries and information needs.

\subsection{Effect on Performance}
The research on query variation is indispensable because it is firmly established that such variations have a detrimental impact on the efficacy of Language Model Models (LLMs) in IR. Numerous studies, including those by Characterbert et al. \cite{characterbert}, Chen et al. \cite{chen}, and Ribeiro et al. \cite{ribeiro}, have consistently shown that text variations lead to underperforming IR systems.

A noteworthy contribution comes from Penha et al.'s study \cite{penha2022}, which modelled seven approaches to ranking using two datasets. Their findings demonstrate that when faced with four different query variations, the effectiveness of IR systems dropped by an average of 20\%. This underscores the pressing need for further research and developing strategies to facilitate the existing models to handle query variations better.

Additionally, Lu et al.'s research \cite{lu} on relevance modelling with multiple queries provides valuable insights into how the utilisation of query variations can significantly outperform the use of a single query. Their experiments involved fusion at the term, query, and document levels, introducing a novel concept in the field that shows promising results. By incorporating query variations, LLMs can capture the nuances and complexities of user queries and information needs more effectively, resulting in more accurate and relevant search results, as supported by several recent studies \cite{penha2022, characterbert, zendel, lu}. Moreover, Zuccon et al.'s study \cite{zuccon} is notable for examining the role of query variations in comparing system effectiveness and proposing a framework that explicitly incorporates query variations. Unlike similar studies, their analysis takes into account not only the mean efficacy of the system but also its variance across different query variations and topics, revealing a significant impact of query variations on comparing other systems.

\subsection{Methods of Generation}
Recent research has explored various methods for generating queries that align more closely with user intent and effectively manage variations. These methods include data augmentation, fusion, query reformulation, and adversarial training, as investigated by Lu et al. \cite{lu}, Bailey et al. \cite{bailey}, and Benham et al. \cite{benham}. Bailey et al. emphasise the critical role of query formulation in studying query variability effectively within LLM evaluation. Benham et al. delves into constructing query variations using fusion and weighted random sampling processes, achieving retrieval effectiveness that competes with state-of-the-art approaches.

In summary, the recent literature on query variation in IR highlights the critical role of benchmark datasets, the potential impact of query variations on system performance, the promising approach of query generation techniques, and the need for more appropriate evaluation metrics. Accounting for query variations is essential for developing effective IR systems that can accurately understand and respond to the diverse information needs of users.