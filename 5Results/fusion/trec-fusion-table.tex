\begin{table}[ht]
\centering
\caption{Effectiveness (nDCG@10) of different methods for TREC-DL-2019 when employing rank fusion (RRF) of the rankings obtained by using different sets of queries.}
\label{tab:trec-fusion-table}
\begin{tabularx}{\columnwidth}{l|X|X|X|X|X|X|X}

\textbf{} & \textbf{\textbf{BM25}} & \textbf{\textbf{RM3}} & \textbf{KNRM} & \textbf{CKNRM} & \textbf{EPIC} & \textbf{BERT} & \textbf{T5} \\ \hline
Original Query   & 0.4795 & 0.5156 & 0.4941 & 0.4931 & 0.624  & 0.6358 & 0.6998 \\ \hline
$RRF_{Misspelling}$ & 0.3035 & 0.3073 & 0.3175 & 0.3175 & 0.3838 & 0.3941 & 0.4636 \\
$RRF_{Naturality}$  & 0.4744 & 0.4972 & 0.4668 & 0.464  & 0.5925 & 0.6005 & 0.6643 \\
$RRF_{Paraphrase}$  & 0.4742 & 0.4865 & 0.4883 & 0.433  & 0.5847 & 0.577  & 0.6616 \\
$RRF_{Synonym}$     & 0.4247 & 0.4066 & 0.4117 & 0.4048 & 0.4975 & 0.4902 & 0.5632 \\
$RRF_{All}$         & 0.4752 & 0.4958 & 0.4965 & 0.4951 & 0.5908 & 0.5939 & 0.6442 \\ \hline
Best Query       & 0.6964 & 0.5401 & 0.6116 & 0.6983 & 0.6939 & 0.6784 & 0.7598
\end{tabularx}
\end{table}
