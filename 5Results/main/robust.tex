\section{Robustness to Query Variations}
\subsection{Research Question 1: Reproduction}
A comprehensive set of experiments was conducted using the same datasets, models, and methodologies to evaluate the robustness of retrieval pipelines to query variations and investigate the extent to which the original study's findings can be successfully reproduced. The key results of the reproduction study are presented in this section, structured to mirror the organisation of the original study's results.

The first objective was to assess the robustness of different ranking models to query variations, categorised into lexical traditional models (Trad), neural ranking models (NN), and transformer-based language models (TNN). The effectiveness of these models, when original queries were replaced with their corresponding query variations, was determined. The results are outlined in \tab{tab:ant-main-table} for the ANTIQUE and \tab{tab:trec-main-table} for the TREC-DL-2019 datasets, respectively. These tables display the resulting nDCG@10 value for each method and model, grouped by their variation category.

\input{5Results/main/ant-main-table}
\input{5Results/main/trec-main-table}

The findings reveal a substantial degree of alignment with the original study. The experiments consistently demonstrate a statistically significant decline in retrieval effectiveness when various query variations and model combinations are considered. For the TREC-DL-2019 dataset, this reduction is observed in 41 out of 70 instances, representing a slight deviation from the original study's results by 8 cases. Similarly, for the ANTIQUE dataset, a decrease in effectiveness is recorded in 51 instances, deviating by merely 3 cases from the original study's outcomes. Furthermore, the analysis of valid queries illustrates that, on average, the models experience a reduction in effectiveness of 22.02\% for TREC-DL-2019, a figure marginally distinct from the original study's 20.62\%. For ANTIQUE, the average decline amounts to 17.92\%, contrasting with the original study's 19.21\%. These consistent trends reaffirm the original study's pivotal assertion that retrieval pipelines exhibit limited robustness in the face of query variations. These outcomes substantiate previous evidence suggesting that query variations introduce considerable variability into diverse information retrieval systems.

Discrepancies emerge when comparing specific values between this study and the original research. On average, the discrepancies are minor, less than 0.08, particularly evident in the results for the BERT model. One potential explanation for these differences is the value of training iterations. Although the exact training iteration value used in the original study remains undisclosed, this reproduction study had to make do with a value of 50 due to computational constraints. Notably, these minor deviations in results underline the intricate nature of replication work, where subtle variations in experimental settings, environmental conditions, or data pre-processing procedures can lead to distinctions in experimental outcomes.

The replication study effectively addresses Research Question 1 (RQ1). Through a meticulous replication of experiments and methodologies from the original study, critical insights have been offered, and the original findings fortified. The consistently corroborated results from the reproduction study underscore the limited robustness of retrieval pipelines in the face of query variations, consequently reinforcing the pivotal conclusion of the original study. This coherence in findings between the two independent studies signifies the robustness and broader applicability of the original study's observations. This outcome robustly attests to the validity of the research outcomes in the original study's context and their broader relevance, accentuating the significant role of query variations in evaluating retrieval systems.

\subsection{Research Question 2: Expansion}
The outcomes of this extended study provide a broader perspective on the conclusions derived from the original study. To assess the generalisability of the original findings, the study replicated the experiments using the additional DL-TYPO dataset. Furthermore, in line with the original research's assertion regarding the diminished robustness of neural ranking models to query variations, even in the context of contemporary collections, DL-TYPO emerged as an apt candidate for testing the transference of this inquiry. The selection of DL-TYPO was motivated by its novel and unexplored nature, thereby substantiating its suitability for broadening the research endeavour.

\input{5Results/main/typo-main-table}

\tab{tab:typo-main-table} displays the results for the DL-TYPO dataset, applying the same models and methodologies used in the original study. The primary objective was to ascertain if the conclusions regarding retrieval pipeline robustness held when confronted with this real query dataset. This table shows the resulting nDCG@10 value for each method and model, grouped by their variation category.

As anticipated, the results closely paralleled the original research, albeit with a discernibly diminished effectiveness. A pronounced reduction in the retrieval models' performance manifested across most query variation scenarios, thus substantiating the initial investigation's findings. Notably, the extent of the overall effectiveness decline, averaging at 31.81\%, surpassed that observed in the context of TREC-DL-2019 and, notably, ANTIQUE. This observation underscores the resilience of the conclusions derived from the original study when transposed to the novel and distinct DL-TYPO dataset. The research's core inferences and insights possess a certain degree of generalisability, albeit with the caveat of nuanced dataset-specific variations in the extent of the observed effects.

The variation in results between the DL-TYPO dataset and the previously studied datasets can be attributed to several underlying factors. Firstly, the DL-TYPO dataset inherently features more query variations (misspellings, specifically) than the synthetically generated counterparts. The diversity and unpredictability of these authentic query variations could pose a more substantial challenge to the robustness of retrieval pipelines, leading to a potentially higher decline in effectiveness. Furthermore, the specific characteristics of the queries in the DL-TYPO dataset, such as the frequency and types of typos, differ significantly from those in the other datasets. These idiosyncrasies interact with the retrieval models and the query variations in a distinct manner, further contributing to variations in observed effectiveness.
Additionally mentioned in the original study, dataset-specific properties, such as query lengths, language styles, and topical diversity, play a role in the differing outcomes. Lastly, the performance of retrieval models can also be influenced by the dataset's composition in terms of judged and unjudged documents. If the query variations in the DL-TYPO dataset significantly increase unjudged documents' ranking highly, this could impact the perceived effectiveness, even if some of these documents might be contextually relevant.

\subsection{Robustness by Query Variation Category}
The approach employed in the original study was replicated to examine the impact of different query variation categories on model effectiveness. Four distinct categories, namely misspellings, paraphrasing, naturality, and ordering, were systematically assigned to various query variations. The distribution of nDCG@10 $\Delta$ values across all models and variation categories is illustrated in the accompanying \fig{fig:dist-plots}, and these results are substantiated by the mean nDCG@10 $\Delta$ values per category and dataset, as depicted in \fig{fig:cat-changes}.

\input{5Results/main/distributions.tex}
\input{5Results/main/cat-changes-fig.tex}

The findings highlight that, on average, the most pronounced negative impact was observed in misspelling variations. These variations recorded nDCG@10 $\Delta$ values of -0.08 for ANTIQUE, -0.25 for TREC-DL-2019, and -0.11 for DL-TYPO. These outcomes align with the observations made in the original study, further reinforcing the notion that misspelling variations consistently undermine retrieval effectiveness. In contrast, variations falling under the paraphrasing and naturality categories exhibited more modest nDCG@10 $\Delta$ values, with specific queries displaying a positive effect, mitigating the overall decline in retrieval performance. Notably, ordering variations had a negligible impact on traditional models, owing to their inherent nature as bag-of-words models, resulting in an average nDCG@10 $\Delta$ close to zero. This comprehensive evaluation elucidates the divergent impacts of distinct query variation categories on the efficacy of retrieval models. The consistent underperformance of DL-TYPO compared to ANTIQUE and TREC-DL-2019 further validates the notion that misspellings lead to diminished retrieval effectiveness. This is evident when comparing the original query values in \tab{tab:typo-main-table} with those in \tab{tab:ant-main-table} and \tab{tab:trec-main-table}.

A closer look at the specific methods within each category, \tab{tab:m-egs} provides an overview of how these methods affect nDCG@10 $\Delta$ when applied to an example query from DL-TYPO. Notably, the T5DescToTitle method results in a significant 75\% reduction in nDCG@10. Other methods, such as NeighbCharSwap, RandomCharSub, and QWERTYCharSub, lead to approximately 49 decreases in nDCG@10, underscoring their detrimental impact on query variations. This analysis emphasises the importance of carefully selecting variation methods, as their effects can vary significantly and have a substantial impact on the quality of retrieval results.

\input{5Results/main/m-egs-table}