\section{Assessment of Generator Quality}
Due to the automated nature of query generation methods, the quality of the generated query variations is a focal point of this research. A thorough evaluation of the generated query variations' quality was carried out, involving manual annotations for 1,371 pairs of {original query, developed query variation} extracted from the test sets of TREC-DL-2019 and ANTIQUE. This rigorous quality assessment aims to select only those query variations that align with the intended query variation categories, enhancing the overall reliability of the study's findings. For the sake of consistency with the original study, each pair of the ANTIQUE dataset was re-evaluated and compared to the judgements made by the original authors.

This approach was meticulously followed in this study when analysing the variations generated for the DL-TYPO dataset. This dataset comprises 60 queries, with each query undergoing query variation generation. The methods employed for query variation generation were consistent with those of the original study, albeit with minor adjustments due to the absence of sourcing from IR\_datasets. The process can be briefly summarised as follows:

\begin{enumerate}
\item \textbf{Query Variations Generation:} Following the methodology utilised in the original study, a query variation, denoted as qˆ, was generated for each query, q, using each method, M.
\item \textbf{Variation Editing:} Punctuation and capital letters were consistently eliminated from the query variations, adhering to the procedures delineated in the original study.
\item \textbf{Automatic Annotation:} All variations resulting from misspelling and ordering were automatically categorised as valid, as these transformations are considered rule-based and maintain the same query semantics. Additionally, any transformations leading to a query identical to the input query (qˆ = M(q) = q) were automatically classified as invalid.
\item \textbf{Manual Annotation:} For the remaining 320 query-variation pairs, manual annotation was conducted, following criteria analogous to those employed in the original study. These criteria encompassed two primary considerations: (I) Whether qˆ retained the same semantics as q and (II) Whether the syntax difference between q and qˆ could be attributed to category C.
\end{enumerate}

Furthermore, the observations arising from DL-TYPO variation annotations indicated several parallels with the original study, as well as novel insights:

\subsection{Parallel Findings (RQ1):}
\begin{itemize}
    \item[(I)] \textbf{T5DescToTitle Effects:} The T5DescToTitle method occasionally resulted in the removal of essential query terms, thereby altering query semantics (e.g. "things to do when broed" to "broeding" (T5DescToTitle), "how money does the show seinfled make" to "seinfled money" (T5DescToTitle)) (total of 5 occurrences).
    \item[(II)] \textbf{Replicating Identical Queries:} The BackTranslation and T5QQP methods were found to generate identical copies of the input query, which were automatically labelled as invalid (a total of 56 occurrences).
    \item[(III)] \textbf{Inaccurate Synonym Replacements:} Transformations that replaced words with presumed synonyms, such as WordEmbedSynSwap and WordNetSynSwap, sometimes introduced words that were not synonymous in the query context (e.g. "how do i clean my computor monitor screen" to "how do i clean my computor supervises screen" (WordEmbedSynSwap), "what kind of medicine is zytec" to "what kind of music is zytec" (WordNetSynSwap)). (total of 24 occurrences).
\end{itemize}
\subsection{Novel Findings (RQ2):} \label{sec:novelFindings}
\begin{itemize}
    \item[(IV)] \textbf{Proper Noun Alterations:} An expansion of the previous observation revealed that transformations occasionally replaced proper nouns or titles, causing them to no longer refer to the correct entity (e.g. "facts about chirs brown" to "facts about chirs chocolate-brown" (WordNetSynSwap), "singer axel rose" to "singer axel soars" (WordEmbedSynSwap)) (total of 11 occurrences).
    \item[(V)] \textbf{Substituting Non-English Words:} WordEmbedSynSwap replaced a word with its equivalent in another language (e.g. "flee market buildings" to "flee mercado buildings" (WordEmbedSynSwap)). This does not occur in the other datasets (total of 1 occurrence).
    \item[(VI)] \textbf{Misspelling Compounding:} Synonym and paraphrasing-related methods were observed to compound misspellings, generating queries with different semantics, despite the method functioning correctly (e.g. "medal taste in mouth symptoms" to "decoration taste in mouth symptoms" (WordNetSynSwap), "flee market buildings" to "the escape market building" (BackTranslation)) (total of 14 occurrences).
    \item[(VII)] \textbf{Fixing Spelling Errors:} Interestingly, five of the ten query variation methods were identified as fixing the original spelling errors (e.g. "alchol and drug rehab" to "alcohol and drug rehabilitation" (BackTranslation), "car accident lawers" to "what are car accident lawyers" (T5QQP), "los angelel unified school district" to "los angeles unified school" (T5DescToTitle)). BackTranslation was the method most often responsible for this correction (occurring 14 times), as well as T5QQP (7 times) (total of 25 occurrences). 
\end{itemize}

\input{5Results/valid-count-table.tex}

After rigorous manual annotation, 477 valid query-variation pairs for the DL-TYPO dataset were identified and retained for further analysis. These pairs were meticulously scrutinised based on their semantic integrity and alignment with the original query, providing valuable insights into the effectiveness and challenges posed by various query variation methods. The summarised results of this annotation process for all three datasets, as presented in Table \ref{tab:valid-vars}, offer valuable insights into the performance of ten query variation methods across the three distinct datasets.

RandomOrderSwap, NeighbCharSwap, and QWERTYCharSub consistently produced 100\% valid variations for the DL-TYPO and ANTIQUE datasets, showcasing their robustness in creating semantically sound queries. In the TREC-DL-2019 dataset, these methods also performed well, with QWERTYCharSub achieving a slightly lower but still notable percentage of 97.7\%.

RandomCharSub achieved high percentages of valid variations, reaching 98.3\% for the DL-TYPO dataset and 98.5\% for ANTIQUE, while slightly lagging for the TREC-DL-2019 dataset with 97.7\%. RemoveStopWords showed variable results, with a lower success rate of 66.7\% for the DL-TYPO dataset. However, it consistently achieved a high success rate of 99.5\% for the ANTIQUE dataset and a respectable 86.0\% for the TREC-DL-2019 dataset.

T5DescToTitle performed moderately, with a success rate of 70\% for the DL-TYPO dataset, 68\% for ANTIQUE, and a more promising 81.4\% for the TREC-DL-2019 dataset. WordEmbedSynSwap, T5QQP, and BackTranslation exhibited varying percentages of valid variations across datasets, with WordEmbedSynSwap and T5QQP showing moderate success rates and BackTranslation achieving slightly lower percentages.

Conversely, WordNetSynSwap consistently produced the lowest percentage of valid variations across all datasets, indicating significant challenges in generating semantically valid query variations using this method. It is essential to acknowledge that variations in the validity of DL-TYPO annotations can be attributed, to some extent, to the change in the annotator's identity. The consistency and reliability of these annotations could be enhanced by adopting the original study's practice of involving multiple annotators for this task. Utilising multiple annotators helps mitigate potential individual annotator biases and contributes to a more comprehensive and well-rounded assessment of the dataset's quality and reliability. This approach aligns with established best practices in dataset curation, ensuring that the dataset remains a robust and valuable resource for research.